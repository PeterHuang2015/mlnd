{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 4:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1003, 1: 963, 2: 1041, 3: 976, 4: 1004, 5: 1021, 6: 1004, 7: 981, 8: 1024, 9: 983}\n",
      "First 20 Labels: [0, 6, 0, 2, 7, 2, 1, 2, 4, 1, 5, 6, 6, 3, 1, 3, 5, 5, 8, 1]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 13 Max Value: 169\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 2 Name: bird\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBD0ANCboAaAxQQ8AjQl6AGjs/wC42Lcq2cSEgQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbbd2345b38>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 4\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    return x/255.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "Look into LabelBinarizer in the preprocessing module of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "# or tf.one_hot\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    minimum, maximum = np.min(x), np.max(x)\n",
    "#     print(minimum, maximum)\n",
    "    oh = np.zeros([len(x), 10])\n",
    "    for i in range(len(x)):\n",
    "        oh[i,x[i]] = 1\n",
    "    return oh\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgv841/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIBS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(shape=[None] + list(image_shape), dtype=tf.float32, name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(dtype=tf.float32, shape=[None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers.\n",
    "\n",
    "** Hint: **\n",
    "\n",
    "When unpacking values as an argument in Python, look into the [unpacking](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists) operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv: 0.03125\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernel size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    in_shape = x_tensor.shape.as_list()\n",
    "    customed_stddev = 1./(in_shape[--1])\n",
    "    print('conv: ' + str(customed_stddev))\n",
    "    kernel = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], in_shape[-1], conv_num_outputs], dtype=tf.float32, stddev=customed_stddev), trainable=True)\n",
    "    bias = tf.Variable(tf.zeros([conv_num_outputs]), trainable=True)\n",
    "#     kernel = tf.Variable(initializer([conv_ksize[0], conv_ksize[1], in_shape[-1], conv_num_outputs]), trainable=True)\n",
    "#     bias = tf.Variable(initializer([conv_num_outputs]), trainable=True)\n",
    "    out = tf.nn.conv2d(x_tensor, kernel, [1, conv_strides[0], conv_strides[1] ,1], padding='VALID')\n",
    "    out = tf.nn.bias_add(out, bias)\n",
    "    out = tf.nn.max_pool(out, ksize=[1, pool_ksize[0], pool_ksize[1], 1], strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "    out = tf.nn.relu(out)\n",
    "    return  out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # why np.prod could handle Dimension data type??\n",
    "    shape = int(np.prod(x_tensor.get_shape()[1:]))\n",
    "    return tf.reshape(x_tensor, [-1, shape])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fully: 0.0078125\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    in_dim = x_tensor.shape.as_list()[-1]\n",
    "    customed_stddev = 1./(in_dim)\n",
    "    print('fully: ' + str(customed_stddev))\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=[in_dim, num_outputs], stddev=customed_stddev), trainable=True)\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]), trainable=True)\n",
    "#     weights = tf.Variable(initializer([in_dim, num_outputs]), trainable=True)\n",
    "#     bias = tf.Variable(initializer([num_outputs]), trainable=True)\n",
    "    out = tf.matmul(x_tensor, weights)\n",
    "    out = tf.nn.bias_add(out, bias)\n",
    "    out = tf.nn.relu(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: 0.0078125\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    in_dim = x_tensor.shape.as_list()[-1]\n",
    "    customed_stddev = 1./(in_dim)\n",
    "    print('out: ' + str(customed_stddev))\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=[in_dim, num_outputs], stddev=customed_stddev), trainable=True)\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]), trainable=True)\n",
    "    out = tf.matmul(x_tensor, weights)\n",
    "    out = tf.nn.bias_add(out, bias)\n",
    "    return out\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv: 0.03125\n",
      "conv: 0.06666666666666667\n",
      "conv: 0.14285714285714285\n",
      "fully: 0.00043402777777777775\n",
      "fully: 0.00390625\n",
      "fully: 0.00390625\n",
      "out: 0.00390625\n",
      "conv: 0.03125\n",
      "conv: 0.06666666666666667\n",
      "conv: 0.14285714285714285\n",
      "fully: 0.00043402777777777775\n",
      "fully: 0.00390625\n",
      "fully: 0.00390625\n",
      "out: 0.00390625\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # add conv layers\n",
    "    conv1 = conv2d_maxpool(x, 64, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    conv2 = conv2d_maxpool(conv1, 128, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "    conv3 = conv2d_maxpool(conv2, 256, (3, 3), (1, 1), (2, 2), (2, 2))\n",
    "\n",
    "    # flatten\n",
    "    flat = flatten(conv3)\n",
    "\n",
    "    # add fully connnected layers\n",
    "    fc1 = fully_conn(flat, 256)\n",
    "    fc2 = fully_conn(fc1, 256)\n",
    "    fc3 = fully_conn(fc2, 256)\n",
    "    fc3 = tf.nn.dropout(fc3, keep_prob)\n",
    "    \n",
    "    # output layer\n",
    "    out = output(fc3, 10)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer, feed_dict={x: feature_batch,\n",
    "                                   y: label_batch,\n",
    "                                   keep_prob: keep_probability})\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shouldn't keep_prob be the same value with which was used for training?\n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    c, a = session.run([cost, accuracy], feed_dict={x: feature_batch,\n",
    "                                                    y: label_batch,\n",
    "                                                    keep_prob: 1.0})\n",
    "    print('current loss: ' + str(c) + ' and accuracy: ' + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tune Parameters\n",
    "epochs = 66\n",
    "batch_size = 64\n",
    "keep_probability = 1.\n",
    "# tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  current loss: 2.23176 and accuracy: 0.1\n",
      "Epoch  2, CIFAR-10 Batch 1:  current loss: 2.04828 and accuracy: 0.125\n",
      "Epoch  3, CIFAR-10 Batch 1:  current loss: 1.97502 and accuracy: 0.25\n",
      "Epoch  4, CIFAR-10 Batch 1:  current loss: 1.92245 and accuracy: 0.25\n",
      "Epoch  5, CIFAR-10 Batch 1:  current loss: 1.89062 and accuracy: 0.25\n",
      "Epoch  6, CIFAR-10 Batch 1:  current loss: 1.72042 and accuracy: 0.4\n",
      "Epoch  7, CIFAR-10 Batch 1:  current loss: 1.59655 and accuracy: 0.375\n",
      "Epoch  8, CIFAR-10 Batch 1:  current loss: 1.48554 and accuracy: 0.425\n",
      "Epoch  9, CIFAR-10 Batch 1:  current loss: 1.41752 and accuracy: 0.45\n",
      "Epoch 10, CIFAR-10 Batch 1:  current loss: 1.16367 and accuracy: 0.525\n",
      "Epoch 11, CIFAR-10 Batch 1:  current loss: 1.21534 and accuracy: 0.55\n",
      "Epoch 12, CIFAR-10 Batch 1:  current loss: 1.12641 and accuracy: 0.55\n",
      "Epoch 13, CIFAR-10 Batch 1:  current loss: 1.01401 and accuracy: 0.55\n",
      "Epoch 14, CIFAR-10 Batch 1:  current loss: 0.81302 and accuracy: 0.675\n",
      "Epoch 15, CIFAR-10 Batch 1:  current loss: 0.725232 and accuracy: 0.7\n",
      "Epoch 16, CIFAR-10 Batch 1:  current loss: 0.644264 and accuracy: 0.7\n",
      "Epoch 17, CIFAR-10 Batch 1:  current loss: 0.649582 and accuracy: 0.725\n",
      "Epoch 18, CIFAR-10 Batch 1:  current loss: 0.656875 and accuracy: 0.775\n",
      "Epoch 19, CIFAR-10 Batch 1:  current loss: 0.60429 and accuracy: 0.75\n",
      "Epoch 20, CIFAR-10 Batch 1:  current loss: 0.400827 and accuracy: 0.9\n",
      "Epoch 21, CIFAR-10 Batch 1:  current loss: 0.302588 and accuracy: 0.85\n",
      "Epoch 22, CIFAR-10 Batch 1:  current loss: 0.184586 and accuracy: 0.95\n",
      "Epoch 23, CIFAR-10 Batch 1:  current loss: 0.202313 and accuracy: 0.9\n",
      "Epoch 24, CIFAR-10 Batch 1:  current loss: 0.173031 and accuracy: 0.975\n",
      "Epoch 25, CIFAR-10 Batch 1:  current loss: 0.137429 and accuracy: 0.975\n",
      "Epoch 26, CIFAR-10 Batch 1:  current loss: 0.294961 and accuracy: 0.9\n",
      "Epoch 27, CIFAR-10 Batch 1:  current loss: 0.107312 and accuracy: 0.975\n",
      "Epoch 28, CIFAR-10 Batch 1:  current loss: 0.0543306 and accuracy: 1.0\n",
      "Epoch 29, CIFAR-10 Batch 1:  current loss: 0.232457 and accuracy: 0.95\n",
      "Epoch 30, CIFAR-10 Batch 1:  current loss: 0.110507 and accuracy: 0.95\n",
      "Epoch 31, CIFAR-10 Batch 1:  current loss: 0.0852755 and accuracy: 0.975\n",
      "Epoch 32, CIFAR-10 Batch 1:  current loss: 0.0451452 and accuracy: 1.0\n",
      "Epoch 33, CIFAR-10 Batch 1:  current loss: 0.0407987 and accuracy: 1.0\n",
      "Epoch 34, CIFAR-10 Batch 1:  current loss: 0.0332552 and accuracy: 1.0\n",
      "Epoch 35, CIFAR-10 Batch 1:  current loss: 0.189277 and accuracy: 0.95\n",
      "Epoch 36, CIFAR-10 Batch 1:  current loss: 0.0685324 and accuracy: 1.0\n",
      "Epoch 37, CIFAR-10 Batch 1:  current loss: 0.0231864 and accuracy: 1.0\n",
      "Epoch 38, CIFAR-10 Batch 1:  current loss: 0.0239018 and accuracy: 1.0\n",
      "Epoch 39, CIFAR-10 Batch 1:  current loss: 0.0264373 and accuracy: 1.0\n",
      "Epoch 40, CIFAR-10 Batch 1:  current loss: 0.0217069 and accuracy: 1.0\n",
      "Epoch 41, CIFAR-10 Batch 1:  current loss: 0.02406 and accuracy: 1.0\n",
      "Epoch 42, CIFAR-10 Batch 1:  current loss: 0.0156649 and accuracy: 1.0\n",
      "Epoch 43, CIFAR-10 Batch 1:  current loss: 0.025769 and accuracy: 1.0\n",
      "Epoch 44, CIFAR-10 Batch 1:  current loss: 0.0171561 and accuracy: 1.0\n",
      "Epoch 45, CIFAR-10 Batch 1:  current loss: 0.0221659 and accuracy: 1.0\n",
      "Epoch 46, CIFAR-10 Batch 1:  current loss: 0.0206904 and accuracy: 1.0\n",
      "Epoch 47, CIFAR-10 Batch 1:  current loss: 0.00765598 and accuracy: 1.0\n",
      "Epoch 48, CIFAR-10 Batch 1:  current loss: 0.0187953 and accuracy: 1.0\n",
      "Epoch 49, CIFAR-10 Batch 1:  current loss: 0.0169484 and accuracy: 1.0\n",
      "Epoch 50, CIFAR-10 Batch 1:  current loss: 0.00528013 and accuracy: 1.0\n",
      "Epoch 51, CIFAR-10 Batch 1:  current loss: 0.0217319 and accuracy: 1.0\n",
      "Epoch 52, CIFAR-10 Batch 1:  current loss: 0.0121574 and accuracy: 1.0\n",
      "Epoch 53, CIFAR-10 Batch 1:  current loss: 0.0041932 and accuracy: 1.0\n",
      "Epoch 54, CIFAR-10 Batch 1:  current loss: 0.0164179 and accuracy: 1.0\n",
      "Epoch 55, CIFAR-10 Batch 1:  current loss: 0.0535112 and accuracy: 0.975\n",
      "Epoch 56, CIFAR-10 Batch 1:  current loss: 0.00363833 and accuracy: 1.0\n",
      "Epoch 57, CIFAR-10 Batch 1:  current loss: 0.00978945 and accuracy: 1.0\n",
      "Epoch 58, CIFAR-10 Batch 1:  current loss: 0.00274238 and accuracy: 1.0\n",
      "Epoch 59, CIFAR-10 Batch 1:  current loss: 0.00713978 and accuracy: 1.0\n",
      "Epoch 60, CIFAR-10 Batch 1:  current loss: 0.00317029 and accuracy: 1.0\n",
      "Epoch 61, CIFAR-10 Batch 1:  current loss: 0.00486767 and accuracy: 1.0\n",
      "Epoch 62, CIFAR-10 Batch 1:  current loss: 0.848221 and accuracy: 0.975\n",
      "Epoch 63, CIFAR-10 Batch 1:  current loss: 0.0217096 and accuracy: 1.0\n",
      "Epoch 64, CIFAR-10 Batch 1:  current loss: 0.00565692 and accuracy: 1.0\n",
      "Epoch 65, CIFAR-10 Batch 1:  current loss: 0.00501608 and accuracy: 1.0\n",
      "Epoch 66, CIFAR-10 Batch 1:  current loss: 0.00327986 and accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        \n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add tensorboard inspection but it didn't work\n",
    "#     tf.summary.scalar('conv1-min', tf.reduce_mean(conv1))\n",
    "#     merged = tf.summary.merge_all()\n",
    "#     train_writer = tf.summary.FileWriter('./train',\n",
    "#                                       sess.graph)\n",
    "#     summary, _ = sess.run([merged, optimizer], \n",
    "#                                   feed_dict={x: batch_features,\n",
    "#                                              y: batch_labels,\n",
    "#                                              keep_prob: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  current loss: 2.21478 and accuracy: 0.175\n",
      "Epoch  1, CIFAR-10 Batch 2:  current loss: 1.95506 and accuracy: 0.275\n",
      "Epoch  1, CIFAR-10 Batch 3:  current loss: 1.57402 and accuracy: 0.35\n",
      "Epoch  1, CIFAR-10 Batch 4:  current loss: 1.78399 and accuracy: 0.2\n",
      "Epoch  1, CIFAR-10 Batch 5:  current loss: 1.5282 and accuracy: 0.4\n",
      "Epoch  2, CIFAR-10 Batch 1:  current loss: 1.71458 and accuracy: 0.375\n",
      "Epoch  2, CIFAR-10 Batch 2:  current loss: 1.49556 and accuracy: 0.475\n",
      "Epoch  2, CIFAR-10 Batch 3:  current loss: 1.25962 and accuracy: 0.525\n",
      "Epoch  2, CIFAR-10 Batch 4:  current loss: 1.39447 and accuracy: 0.45\n",
      "Epoch  2, CIFAR-10 Batch 5:  current loss: 1.09869 and accuracy: 0.7\n",
      "Epoch  3, CIFAR-10 Batch 1:  current loss: 1.23534 and accuracy: 0.55\n",
      "Epoch  3, CIFAR-10 Batch 2:  current loss: 1.04118 and accuracy: 0.6\n",
      "Epoch  3, CIFAR-10 Batch 3:  current loss: 0.72127 and accuracy: 0.7\n",
      "Epoch  3, CIFAR-10 Batch 4:  current loss: 1.05622 and accuracy: 0.575\n",
      "Epoch  3, CIFAR-10 Batch 5:  current loss: 0.787375 and accuracy: 0.75\n",
      "Epoch  4, CIFAR-10 Batch 1:  current loss: 0.974385 and accuracy: 0.55\n",
      "Epoch  4, CIFAR-10 Batch 2:  current loss: 0.794788 and accuracy: 0.7\n",
      "Epoch  4, CIFAR-10 Batch 3:  current loss: 0.521122 and accuracy: 0.85\n",
      "Epoch  4, CIFAR-10 Batch 4:  current loss: 0.733676 and accuracy: 0.7\n",
      "Epoch  4, CIFAR-10 Batch 5:  current loss: 0.707664 and accuracy: 0.775\n",
      "Epoch  5, CIFAR-10 Batch 1:  current loss: 0.721512 and accuracy: 0.775\n",
      "Epoch  5, CIFAR-10 Batch 2:  current loss: 0.702604 and accuracy: 0.725\n",
      "Epoch  5, CIFAR-10 Batch 3:  current loss: 0.384042 and accuracy: 0.925\n",
      "Epoch  5, CIFAR-10 Batch 4:  current loss: 0.5333 and accuracy: 0.8\n",
      "Epoch  5, CIFAR-10 Batch 5:  current loss: 0.493299 and accuracy: 0.85\n",
      "Epoch  6, CIFAR-10 Batch 1:  current loss: 0.548541 and accuracy: 0.825\n",
      "Epoch  6, CIFAR-10 Batch 2:  current loss: 0.643424 and accuracy: 0.775\n",
      "Epoch  6, CIFAR-10 Batch 3:  current loss: 0.354788 and accuracy: 0.95\n",
      "Epoch  6, CIFAR-10 Batch 4:  current loss: 0.397006 and accuracy: 0.875\n",
      "Epoch  6, CIFAR-10 Batch 5:  current loss: 0.400648 and accuracy: 0.875\n",
      "Epoch  7, CIFAR-10 Batch 1:  current loss: 0.458417 and accuracy: 0.85\n",
      "Epoch  7, CIFAR-10 Batch 2:  current loss: 0.397733 and accuracy: 0.85\n",
      "Epoch  7, CIFAR-10 Batch 3:  current loss: 0.265156 and accuracy: 0.925\n",
      "Epoch  7, CIFAR-10 Batch 4:  current loss: 0.369565 and accuracy: 0.825\n",
      "Epoch  7, CIFAR-10 Batch 5:  current loss: 0.277025 and accuracy: 0.925\n",
      "Epoch  8, CIFAR-10 Batch 1:  current loss: 0.325787 and accuracy: 0.875\n",
      "Epoch  8, CIFAR-10 Batch 2:  current loss: 0.30658 and accuracy: 0.9\n",
      "Epoch  8, CIFAR-10 Batch 3:  current loss: 0.205741 and accuracy: 0.95\n",
      "Epoch  8, CIFAR-10 Batch 4:  current loss: 0.315572 and accuracy: 0.9\n",
      "Epoch  8, CIFAR-10 Batch 5:  current loss: 0.179448 and accuracy: 0.95\n",
      "Epoch  9, CIFAR-10 Batch 1:  current loss: 0.17061 and accuracy: 0.95\n",
      "Epoch  9, CIFAR-10 Batch 2:  current loss: 0.204235 and accuracy: 0.925\n",
      "Epoch  9, CIFAR-10 Batch 3:  current loss: 0.132994 and accuracy: 0.95\n",
      "Epoch  9, CIFAR-10 Batch 4:  current loss: 0.232537 and accuracy: 0.95\n",
      "Epoch  9, CIFAR-10 Batch 5:  current loss: 0.141674 and accuracy: 0.925\n",
      "Epoch 10, CIFAR-10 Batch 1:  current loss: 0.1693 and accuracy: 0.925\n",
      "Epoch 10, CIFAR-10 Batch 2:  current loss: 0.181713 and accuracy: 0.925\n",
      "Epoch 10, CIFAR-10 Batch 3:  current loss: 0.182106 and accuracy: 0.95\n",
      "Epoch 10, CIFAR-10 Batch 4:  current loss: 0.162565 and accuracy: 0.975\n",
      "Epoch 10, CIFAR-10 Batch 5:  current loss: 0.0789794 and accuracy: 1.0\n",
      "Epoch 11, CIFAR-10 Batch 1:  current loss: 0.0923672 and accuracy: 0.975\n",
      "Epoch 11, CIFAR-10 Batch 2:  current loss: 0.0998518 and accuracy: 1.0\n",
      "Epoch 11, CIFAR-10 Batch 3:  current loss: 0.0955177 and accuracy: 1.0\n",
      "Epoch 11, CIFAR-10 Batch 4:  current loss: 0.0882726 and accuracy: 0.975\n",
      "Epoch 11, CIFAR-10 Batch 5:  current loss: 0.07533 and accuracy: 0.975\n",
      "Epoch 12, CIFAR-10 Batch 1:  current loss: 0.0717146 and accuracy: 1.0\n",
      "Epoch 12, CIFAR-10 Batch 2:  current loss: 0.0618198 and accuracy: 0.975\n",
      "Epoch 12, CIFAR-10 Batch 3:  current loss: 0.0596704 and accuracy: 1.0\n",
      "Epoch 12, CIFAR-10 Batch 4:  current loss: 0.0689238 and accuracy: 0.975\n",
      "Epoch 12, CIFAR-10 Batch 5:  current loss: 0.0760567 and accuracy: 1.0\n",
      "Epoch 13, CIFAR-10 Batch 1:  current loss: 0.0738578 and accuracy: 0.975\n",
      "Epoch 13, CIFAR-10 Batch 2:  current loss: 0.0521395 and accuracy: 0.975\n",
      "Epoch 13, CIFAR-10 Batch 3:  current loss: 0.0430257 and accuracy: 1.0\n",
      "Epoch 13, CIFAR-10 Batch 4:  current loss: 0.0668534 and accuracy: 1.0\n",
      "Epoch 13, CIFAR-10 Batch 5:  current loss: 0.0487519 and accuracy: 1.0\n",
      "Epoch 14, CIFAR-10 Batch 1:  current loss: 0.0470174 and accuracy: 1.0\n",
      "Epoch 14, CIFAR-10 Batch 2:  current loss: 0.0726434 and accuracy: 1.0\n",
      "Epoch 14, CIFAR-10 Batch 3:  current loss: 0.0878221 and accuracy: 0.975\n",
      "Epoch 14, CIFAR-10 Batch 4:  current loss: 0.031772 and accuracy: 1.0\n",
      "Epoch 14, CIFAR-10 Batch 5:  current loss: 0.178246 and accuracy: 0.95\n",
      "Epoch 15, CIFAR-10 Batch 1:  current loss: 0.0437772 and accuracy: 1.0\n",
      "Epoch 15, CIFAR-10 Batch 2:  current loss: 0.121677 and accuracy: 0.975\n",
      "Epoch 15, CIFAR-10 Batch 3:  current loss: 0.0603286 and accuracy: 1.0\n",
      "Epoch 15, CIFAR-10 Batch 4:  current loss: 0.0480294 and accuracy: 1.0\n",
      "Epoch 15, CIFAR-10 Batch 5:  current loss: 0.0274371 and accuracy: 1.0\n",
      "Epoch 16, CIFAR-10 Batch 1:  current loss: 0.0299435 and accuracy: 1.0\n",
      "Epoch 16, CIFAR-10 Batch 2:  current loss: 0.0274699 and accuracy: 1.0\n",
      "Epoch 16, CIFAR-10 Batch 3:  current loss: 0.0454703 and accuracy: 0.975\n",
      "Epoch 16, CIFAR-10 Batch 4:  current loss: 0.0480289 and accuracy: 0.975\n",
      "Epoch 16, CIFAR-10 Batch 5:  current loss: 0.0321337 and accuracy: 1.0\n",
      "Epoch 17, CIFAR-10 Batch 1:  current loss: 0.0196585 and accuracy: 1.0\n",
      "Epoch 17, CIFAR-10 Batch 2:  current loss: 0.0180659 and accuracy: 1.0\n",
      "Epoch 17, CIFAR-10 Batch 3:  current loss: 0.0529557 and accuracy: 1.0\n",
      "Epoch 17, CIFAR-10 Batch 4:  current loss: 0.00928642 and accuracy: 1.0\n",
      "Epoch 17, CIFAR-10 Batch 5:  current loss: 0.0113778 and accuracy: 1.0\n",
      "Epoch 18, CIFAR-10 Batch 1:  current loss: 0.0263889 and accuracy: 1.0\n",
      "Epoch 18, CIFAR-10 Batch 2:  current loss: 0.0289697 and accuracy: 1.0\n",
      "Epoch 18, CIFAR-10 Batch 3:  current loss: 0.0363287 and accuracy: 1.0\n",
      "Epoch 18, CIFAR-10 Batch 4:  current loss: 0.0205945 and accuracy: 1.0\n",
      "Epoch 18, CIFAR-10 Batch 5:  current loss: 0.0146347 and accuracy: 1.0\n",
      "Epoch 19, CIFAR-10 Batch 1:  current loss: 0.0210679 and accuracy: 1.0\n",
      "Epoch 19, CIFAR-10 Batch 2:  current loss: 0.0192763 and accuracy: 1.0\n",
      "Epoch 19, CIFAR-10 Batch 3:  current loss: 0.0219216 and accuracy: 1.0\n",
      "Epoch 19, CIFAR-10 Batch 4:  current loss: 0.0201637 and accuracy: 1.0\n",
      "Epoch 19, CIFAR-10 Batch 5:  current loss: 0.00654424 and accuracy: 1.0\n",
      "Epoch 20, CIFAR-10 Batch 1:  current loss: 0.0220083 and accuracy: 1.0\n",
      "Epoch 20, CIFAR-10 Batch 2:  current loss: 0.0130186 and accuracy: 1.0\n",
      "Epoch 20, CIFAR-10 Batch 3:  current loss: 0.0171187 and accuracy: 1.0\n",
      "Epoch 20, CIFAR-10 Batch 4:  current loss: 0.019709 and accuracy: 1.0\n",
      "Epoch 20, CIFAR-10 Batch 5:  current loss: 0.00874799 and accuracy: 1.0\n",
      "Epoch 21, CIFAR-10 Batch 1:  current loss: 0.0161135 and accuracy: 1.0\n",
      "Epoch 21, CIFAR-10 Batch 2:  current loss: 0.0104318 and accuracy: 1.0\n",
      "Epoch 21, CIFAR-10 Batch 3:  current loss: 0.00793804 and accuracy: 1.0\n",
      "Epoch 21, CIFAR-10 Batch 4:  current loss: 0.0112964 and accuracy: 1.0\n",
      "Epoch 21, CIFAR-10 Batch 5:  current loss: 0.0543757 and accuracy: 0.975\n",
      "Epoch 22, CIFAR-10 Batch 1:  current loss: 0.0149428 and accuracy: 1.0\n",
      "Epoch 22, CIFAR-10 Batch 2:  current loss: 0.0153853 and accuracy: 1.0\n",
      "Epoch 22, CIFAR-10 Batch 3:  current loss: 0.0171335 and accuracy: 1.0\n",
      "Epoch 22, CIFAR-10 Batch 4:  current loss: 0.0150714 and accuracy: 1.0\n",
      "Epoch 22, CIFAR-10 Batch 5:  current loss: 0.108264 and accuracy: 0.95\n",
      "Epoch 23, CIFAR-10 Batch 1:  current loss: 0.00903788 and accuracy: 1.0\n",
      "Epoch 23, CIFAR-10 Batch 2:  current loss: 0.00402266 and accuracy: 1.0\n",
      "Epoch 23, CIFAR-10 Batch 3:  current loss: 0.0093864 and accuracy: 1.0\n",
      "Epoch 23, CIFAR-10 Batch 4:  current loss: 0.00533927 and accuracy: 1.0\n",
      "Epoch 23, CIFAR-10 Batch 5:  current loss: 0.0064542 and accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, CIFAR-10 Batch 1:  current loss: 0.00750418 and accuracy: 1.0\n",
      "Epoch 24, CIFAR-10 Batch 2:  current loss: 0.0060112 and accuracy: 1.0\n",
      "Epoch 24, CIFAR-10 Batch 3:  current loss: 0.0140533 and accuracy: 1.0\n",
      "Epoch 24, CIFAR-10 Batch 4:  current loss: 0.00659956 and accuracy: 1.0\n",
      "Epoch 24, CIFAR-10 Batch 5:  current loss: 0.00251491 and accuracy: 1.0\n",
      "Epoch 25, CIFAR-10 Batch 1:  current loss: 0.0129415 and accuracy: 1.0\n",
      "Epoch 25, CIFAR-10 Batch 2:  current loss: 0.00371511 and accuracy: 1.0\n",
      "Epoch 25, CIFAR-10 Batch 3:  current loss: 0.00337392 and accuracy: 1.0\n",
      "Epoch 25, CIFAR-10 Batch 4:  current loss: 0.00369542 and accuracy: 1.0\n",
      "Epoch 25, CIFAR-10 Batch 5:  current loss: 0.00162104 and accuracy: 1.0\n",
      "Epoch 26, CIFAR-10 Batch 1:  current loss: 0.00703085 and accuracy: 1.0\n",
      "Epoch 26, CIFAR-10 Batch 2:  current loss: 0.0358656 and accuracy: 0.975\n",
      "Epoch 26, CIFAR-10 Batch 3:  current loss: 0.00366709 and accuracy: 1.0\n",
      "Epoch 26, CIFAR-10 Batch 4:  current loss: 0.0131066 and accuracy: 1.0\n",
      "Epoch 26, CIFAR-10 Batch 5:  current loss: 0.00297888 and accuracy: 1.0\n",
      "Epoch 27, CIFAR-10 Batch 1:  current loss: 0.0366987 and accuracy: 0.975\n",
      "Epoch 27, CIFAR-10 Batch 2:  current loss: 0.00355448 and accuracy: 1.0\n",
      "Epoch 27, CIFAR-10 Batch 3:  current loss: 0.0137908 and accuracy: 1.0\n",
      "Epoch 27, CIFAR-10 Batch 4:  current loss: 0.0216767 and accuracy: 1.0\n",
      "Epoch 27, CIFAR-10 Batch 5:  current loss: 0.00410911 and accuracy: 1.0\n",
      "Epoch 28, CIFAR-10 Batch 1:  current loss: 0.0237108 and accuracy: 1.0\n",
      "Epoch 28, CIFAR-10 Batch 2:  current loss: 0.00475171 and accuracy: 1.0\n",
      "Epoch 28, CIFAR-10 Batch 3:  current loss: 0.00612399 and accuracy: 1.0\n",
      "Epoch 28, CIFAR-10 Batch 4:  current loss: 0.0122162 and accuracy: 1.0\n",
      "Epoch 28, CIFAR-10 Batch 5:  current loss: 0.00298741 and accuracy: 1.0\n",
      "Epoch 29, CIFAR-10 Batch 1:  current loss: 0.0140385 and accuracy: 1.0\n",
      "Epoch 29, CIFAR-10 Batch 2:  current loss: 0.00660693 and accuracy: 1.0\n",
      "Epoch 29, CIFAR-10 Batch 3:  current loss: 0.00959493 and accuracy: 1.0\n",
      "Epoch 29, CIFAR-10 Batch 4:  current loss: 0.00470914 and accuracy: 1.0\n",
      "Epoch 29, CIFAR-10 Batch 5:  current loss: 0.0055232 and accuracy: 1.0\n",
      "Epoch 30, CIFAR-10 Batch 1:  current loss: 0.0102795 and accuracy: 1.0\n",
      "Epoch 30, CIFAR-10 Batch 2:  current loss: 0.0238643 and accuracy: 1.0\n",
      "Epoch 30, CIFAR-10 Batch 3:  current loss: 0.00331881 and accuracy: 1.0\n",
      "Epoch 30, CIFAR-10 Batch 4:  current loss: 0.0067352 and accuracy: 1.0\n",
      "Epoch 30, CIFAR-10 Batch 5:  current loss: 0.0042867 and accuracy: 1.0\n",
      "Epoch 31, CIFAR-10 Batch 1:  current loss: 0.00777781 and accuracy: 1.0\n",
      "Epoch 31, CIFAR-10 Batch 2:  current loss: 0.000635014 and accuracy: 1.0\n",
      "Epoch 31, CIFAR-10 Batch 3:  current loss: 0.00458043 and accuracy: 1.0\n",
      "Epoch 31, CIFAR-10 Batch 4:  current loss: 0.0171991 and accuracy: 1.0\n",
      "Epoch 31, CIFAR-10 Batch 5:  current loss: 0.004396 and accuracy: 1.0\n",
      "Epoch 32, CIFAR-10 Batch 1:  current loss: 0.00197983 and accuracy: 1.0\n",
      "Epoch 32, CIFAR-10 Batch 2:  current loss: 0.00692578 and accuracy: 1.0\n",
      "Epoch 32, CIFAR-10 Batch 3:  current loss: 0.00407525 and accuracy: 1.0\n",
      "Epoch 32, CIFAR-10 Batch 4:  current loss: 0.00401047 and accuracy: 1.0\n",
      "Epoch 32, CIFAR-10 Batch 5:  current loss: 0.000671435 and accuracy: 1.0\n",
      "Epoch 33, CIFAR-10 Batch 1:  current loss: 0.00447827 and accuracy: 1.0\n",
      "Epoch 33, CIFAR-10 Batch 2:  current loss: 0.00175735 and accuracy: 1.0\n",
      "Epoch 33, CIFAR-10 Batch 3:  current loss: 0.0125144 and accuracy: 1.0\n",
      "Epoch 33, CIFAR-10 Batch 4:  current loss: 0.0279718 and accuracy: 0.975\n",
      "Epoch 33, CIFAR-10 Batch 5:  current loss: 0.00170361 and accuracy: 1.0\n",
      "Epoch 34, CIFAR-10 Batch 1:  current loss: 0.00771251 and accuracy: 1.0\n",
      "Epoch 34, CIFAR-10 Batch 2:  current loss: 0.00249847 and accuracy: 1.0\n",
      "Epoch 34, CIFAR-10 Batch 3:  current loss: 0.0081597 and accuracy: 1.0\n",
      "Epoch 34, CIFAR-10 Batch 4:  current loss: 0.00550731 and accuracy: 1.0\n",
      "Epoch 34, CIFAR-10 Batch 5:  current loss: 0.00190212 and accuracy: 1.0\n",
      "Epoch 35, CIFAR-10 Batch 1:  current loss: 0.00525909 and accuracy: 1.0\n",
      "Epoch 35, CIFAR-10 Batch 2:  current loss: 0.00107064 and accuracy: 1.0\n",
      "Epoch 35, CIFAR-10 Batch 3:  current loss: 0.00802952 and accuracy: 1.0\n",
      "Epoch 35, CIFAR-10 Batch 4:  current loss: 0.00215414 and accuracy: 1.0\n",
      "Epoch 35, CIFAR-10 Batch 5:  current loss: 0.0398743 and accuracy: 0.975\n",
      "Epoch 36, CIFAR-10 Batch 1:  current loss: 0.00331634 and accuracy: 1.0\n",
      "Epoch 36, CIFAR-10 Batch 2:  current loss: 0.000758077 and accuracy: 1.0\n",
      "Epoch 36, CIFAR-10 Batch 3:  current loss: 0.00565931 and accuracy: 1.0\n",
      "Epoch 36, CIFAR-10 Batch 4:  current loss: 0.00950568 and accuracy: 1.0\n",
      "Epoch 36, CIFAR-10 Batch 5:  current loss: 0.00334794 and accuracy: 1.0\n",
      "Epoch 37, CIFAR-10 Batch 1:  current loss: 0.000299402 and accuracy: 1.0\n",
      "Epoch 37, CIFAR-10 Batch 2:  current loss: 0.00294303 and accuracy: 1.0\n",
      "Epoch 37, CIFAR-10 Batch 3:  current loss: 0.0027255 and accuracy: 1.0\n",
      "Epoch 37, CIFAR-10 Batch 4:  current loss: 0.00210434 and accuracy: 1.0\n",
      "Epoch 37, CIFAR-10 Batch 5:  current loss: 0.00246044 and accuracy: 1.0\n",
      "Epoch 38, CIFAR-10 Batch 1:  current loss: 0.000466304 and accuracy: 1.0\n",
      "Epoch 38, CIFAR-10 Batch 2:  current loss: 0.00420989 and accuracy: 1.0\n",
      "Epoch 38, CIFAR-10 Batch 3:  current loss: 0.000649409 and accuracy: 1.0\n",
      "Epoch 38, CIFAR-10 Batch 4:  current loss: 0.00905355 and accuracy: 1.0\n",
      "Epoch 38, CIFAR-10 Batch 5:  current loss: 0.00175322 and accuracy: 1.0\n",
      "Epoch 39, CIFAR-10 Batch 1:  current loss: 0.0018826 and accuracy: 1.0\n",
      "Epoch 39, CIFAR-10 Batch 2:  current loss: 0.0017409 and accuracy: 1.0\n",
      "Epoch 39, CIFAR-10 Batch 3:  current loss: 0.000683969 and accuracy: 1.0\n",
      "Epoch 39, CIFAR-10 Batch 4:  current loss: 0.00118658 and accuracy: 1.0\n",
      "Epoch 39, CIFAR-10 Batch 5:  current loss: 0.00277444 and accuracy: 1.0\n",
      "Epoch 40, CIFAR-10 Batch 1:  current loss: 0.00425377 and accuracy: 1.0\n",
      "Epoch 40, CIFAR-10 Batch 2:  current loss: 0.000300778 and accuracy: 1.0\n",
      "Epoch 40, CIFAR-10 Batch 3:  current loss: 0.00502912 and accuracy: 1.0\n",
      "Epoch 40, CIFAR-10 Batch 4:  current loss: 0.00241664 and accuracy: 1.0\n",
      "Epoch 40, CIFAR-10 Batch 5:  current loss: 0.00493566 and accuracy: 1.0\n",
      "Epoch 41, CIFAR-10 Batch 1:  current loss: 0.00316572 and accuracy: 1.0\n",
      "Epoch 41, CIFAR-10 Batch 2:  current loss: 0.00149782 and accuracy: 1.0\n",
      "Epoch 41, CIFAR-10 Batch 3:  current loss: 0.00687708 and accuracy: 1.0\n",
      "Epoch 41, CIFAR-10 Batch 4:  current loss: 0.00695872 and accuracy: 1.0\n",
      "Epoch 41, CIFAR-10 Batch 5:  current loss: 0.00796204 and accuracy: 1.0\n",
      "Epoch 42, CIFAR-10 Batch 1:  current loss: 0.00636173 and accuracy: 1.0\n",
      "Epoch 42, CIFAR-10 Batch 2:  current loss: 0.00113681 and accuracy: 1.0\n",
      "Epoch 42, CIFAR-10 Batch 3:  current loss: 0.00418043 and accuracy: 1.0\n",
      "Epoch 42, CIFAR-10 Batch 4:  current loss: 0.0104141 and accuracy: 1.0\n",
      "Epoch 42, CIFAR-10 Batch 5:  current loss: 0.00126256 and accuracy: 1.0\n",
      "Epoch 43, CIFAR-10 Batch 1:  current loss: 0.000931644 and accuracy: 1.0\n",
      "Epoch 43, CIFAR-10 Batch 2:  current loss: 0.000687062 and accuracy: 1.0\n",
      "Epoch 43, CIFAR-10 Batch 3:  current loss: 0.00207446 and accuracy: 1.0\n",
      "Epoch 43, CIFAR-10 Batch 4:  current loss: 0.00848808 and accuracy: 1.0\n",
      "Epoch 43, CIFAR-10 Batch 5:  current loss: 0.0153336 and accuracy: 1.0\n",
      "Epoch 44, CIFAR-10 Batch 1:  current loss: 0.000947943 and accuracy: 1.0\n",
      "Epoch 44, CIFAR-10 Batch 2:  current loss: 0.00108323 and accuracy: 1.0\n",
      "Epoch 44, CIFAR-10 Batch 3:  current loss: 0.00341818 and accuracy: 1.0\n",
      "Epoch 44, CIFAR-10 Batch 4:  current loss: 0.000702372 and accuracy: 1.0\n",
      "Epoch 44, CIFAR-10 Batch 5:  current loss: 0.00160973 and accuracy: 1.0\n",
      "Epoch 45, CIFAR-10 Batch 1:  current loss: 0.0139751 and accuracy: 1.0\n",
      "Epoch 45, CIFAR-10 Batch 2:  current loss: 0.00615496 and accuracy: 1.0\n",
      "Epoch 45, CIFAR-10 Batch 3:  current loss: 0.00150124 and accuracy: 1.0\n",
      "Epoch 45, CIFAR-10 Batch 4:  current loss: 0.0165912 and accuracy: 1.0\n",
      "Epoch 45, CIFAR-10 Batch 5:  current loss: 0.00163591 and accuracy: 1.0\n",
      "Epoch 46, CIFAR-10 Batch 1:  current loss: 0.00145524 and accuracy: 1.0\n",
      "Epoch 46, CIFAR-10 Batch 2:  current loss: 0.00528888 and accuracy: 1.0\n",
      "Epoch 46, CIFAR-10 Batch 3:  current loss: 0.000975421 and accuracy: 1.0\n",
      "Epoch 46, CIFAR-10 Batch 4:  current loss: 0.0012382 and accuracy: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, CIFAR-10 Batch 5:  current loss: 0.00059016 and accuracy: 1.0\n",
      "Epoch 47, CIFAR-10 Batch 1:  current loss: 0.000257555 and accuracy: 1.0\n",
      "Epoch 47, CIFAR-10 Batch 2:  current loss: 0.00560254 and accuracy: 1.0\n",
      "Epoch 47, CIFAR-10 Batch 3:  current loss: 0.00331051 and accuracy: 1.0\n",
      "Epoch 47, CIFAR-10 Batch 4:  current loss: 0.00144167 and accuracy: 1.0\n",
      "Epoch 47, CIFAR-10 Batch 5:  current loss: 0.00241363 and accuracy: 1.0\n",
      "Epoch 48, CIFAR-10 Batch 1:  current loss: 0.00350517 and accuracy: 1.0\n",
      "Epoch 48, CIFAR-10 Batch 2:  current loss: 0.00266677 and accuracy: 1.0\n",
      "Epoch 48, CIFAR-10 Batch 3:  current loss: 0.00221579 and accuracy: 1.0\n",
      "Epoch 48, CIFAR-10 Batch 4:  current loss: 0.00164869 and accuracy: 1.0\n",
      "Epoch 48, CIFAR-10 Batch 5:  current loss: 0.00474025 and accuracy: 1.0\n",
      "Epoch 49, CIFAR-10 Batch 1:  current loss: 0.000151151 and accuracy: 1.0\n",
      "Epoch 49, CIFAR-10 Batch 2:  current loss: 0.00127286 and accuracy: 1.0\n",
      "Epoch 49, CIFAR-10 Batch 3:  current loss: 0.00331967 and accuracy: 1.0\n",
      "Epoch 49, CIFAR-10 Batch 4:  current loss: 0.000619353 and accuracy: 1.0\n",
      "Epoch 49, CIFAR-10 Batch 5:  current loss: 0.0060067 and accuracy: 1.0\n",
      "Epoch 50, CIFAR-10 Batch 1:  current loss: 0.00445427 and accuracy: 1.0\n",
      "Epoch 50, CIFAR-10 Batch 2:  current loss: 0.000920077 and accuracy: 1.0\n",
      "Epoch 50, CIFAR-10 Batch 3:  current loss: 0.00206174 and accuracy: 1.0\n",
      "Epoch 50, CIFAR-10 Batch 4:  current loss: 0.00164254 and accuracy: 1.0\n",
      "Epoch 50, CIFAR-10 Batch 5:  current loss: 0.00849703 and accuracy: 1.0\n",
      "Epoch 51, CIFAR-10 Batch 1:  current loss: 0.00350837 and accuracy: 1.0\n",
      "Epoch 51, CIFAR-10 Batch 2:  current loss: 0.00134606 and accuracy: 1.0\n",
      "Epoch 51, CIFAR-10 Batch 3:  current loss: 0.000386393 and accuracy: 1.0\n",
      "Epoch 51, CIFAR-10 Batch 4:  current loss: 0.000574304 and accuracy: 1.0\n",
      "Epoch 51, CIFAR-10 Batch 5:  current loss: 0.00228038 and accuracy: 1.0\n",
      "Epoch 52, CIFAR-10 Batch 1:  current loss: 0.00164974 and accuracy: 1.0\n",
      "Epoch 52, CIFAR-10 Batch 2:  current loss: 0.00263235 and accuracy: 1.0\n",
      "Epoch 52, CIFAR-10 Batch 3:  current loss: 0.00970542 and accuracy: 1.0\n",
      "Epoch 52, CIFAR-10 Batch 4:  current loss: 0.000753539 and accuracy: 1.0\n",
      "Epoch 52, CIFAR-10 Batch 5:  current loss: 0.000619789 and accuracy: 1.0\n",
      "Epoch 53, CIFAR-10 Batch 1:  current loss: 0.00356636 and accuracy: 1.0\n",
      "Epoch 53, CIFAR-10 Batch 2:  current loss: 0.0371725 and accuracy: 0.975\n",
      "Epoch 53, CIFAR-10 Batch 3:  current loss: 0.00298071 and accuracy: 1.0\n",
      "Epoch 53, CIFAR-10 Batch 4:  current loss: 0.00372318 and accuracy: 1.0\n",
      "Epoch 53, CIFAR-10 Batch 5:  current loss: 0.000824243 and accuracy: 1.0\n",
      "Epoch 54, CIFAR-10 Batch 1:  current loss: 0.000296711 and accuracy: 1.0\n",
      "Epoch 54, CIFAR-10 Batch 2:  current loss: 0.00357892 and accuracy: 1.0\n",
      "Epoch 54, CIFAR-10 Batch 3:  current loss: 0.00112166 and accuracy: 1.0\n",
      "Epoch 54, CIFAR-10 Batch 4:  current loss: 0.00187812 and accuracy: 1.0\n",
      "Epoch 54, CIFAR-10 Batch 5:  current loss: 0.000239382 and accuracy: 1.0\n",
      "Epoch 55, CIFAR-10 Batch 1:  current loss: 0.000259857 and accuracy: 1.0\n",
      "Epoch 55, CIFAR-10 Batch 2:  current loss: 0.00232339 and accuracy: 1.0\n",
      "Epoch 55, CIFAR-10 Batch 3:  current loss: 0.000857643 and accuracy: 1.0\n",
      "Epoch 55, CIFAR-10 Batch 4:  current loss: 0.000728466 and accuracy: 1.0\n",
      "Epoch 55, CIFAR-10 Batch 5:  current loss: 0.00394026 and accuracy: 1.0\n",
      "Epoch 56, CIFAR-10 Batch 1:  current loss: 0.00654953 and accuracy: 1.0\n",
      "Epoch 56, CIFAR-10 Batch 2:  current loss: 0.000748909 and accuracy: 1.0\n",
      "Epoch 56, CIFAR-10 Batch 3:  current loss: 0.000185715 and accuracy: 1.0\n",
      "Epoch 56, CIFAR-10 Batch 4:  current loss: 0.00373167 and accuracy: 1.0\n",
      "Epoch 56, CIFAR-10 Batch 5:  current loss: 0.000380412 and accuracy: 1.0\n",
      "Epoch 57, CIFAR-10 Batch 1:  current loss: 0.00939914 and accuracy: 1.0\n",
      "Epoch 57, CIFAR-10 Batch 2:  current loss: 0.00248001 and accuracy: 1.0\n",
      "Epoch 57, CIFAR-10 Batch 3:  current loss: 0.00103814 and accuracy: 1.0\n",
      "Epoch 57, CIFAR-10 Batch 4:  current loss: 0.005586 and accuracy: 1.0\n",
      "Epoch 57, CIFAR-10 Batch 5:  current loss: 0.000457697 and accuracy: 1.0\n",
      "Epoch 58, CIFAR-10 Batch 1:  current loss: 0.00903667 and accuracy: 1.0\n",
      "Epoch 58, CIFAR-10 Batch 2:  current loss: 0.0011782 and accuracy: 1.0\n",
      "Epoch 58, CIFAR-10 Batch 3:  current loss: 0.000857345 and accuracy: 1.0\n",
      "Epoch 58, CIFAR-10 Batch 4:  current loss: 0.00209638 and accuracy: 1.0\n",
      "Epoch 58, CIFAR-10 Batch 5:  current loss: 0.000113258 and accuracy: 1.0\n",
      "Epoch 59, CIFAR-10 Batch 1:  current loss: 0.00114518 and accuracy: 1.0\n",
      "Epoch 59, CIFAR-10 Batch 2:  current loss: 0.000322711 and accuracy: 1.0\n",
      "Epoch 59, CIFAR-10 Batch 3:  current loss: 0.000884133 and accuracy: 1.0\n",
      "Epoch 59, CIFAR-10 Batch 4:  current loss: 0.0066049 and accuracy: 1.0\n",
      "Epoch 59, CIFAR-10 Batch 5:  current loss: 0.00354216 and accuracy: 1.0\n",
      "Epoch 60, CIFAR-10 Batch 1:  current loss: 0.00562396 and accuracy: 1.0\n",
      "Epoch 60, CIFAR-10 Batch 2:  current loss: 0.00247734 and accuracy: 1.0\n",
      "Epoch 60, CIFAR-10 Batch 3:  current loss: 0.00157526 and accuracy: 1.0\n",
      "Epoch 60, CIFAR-10 Batch 4:  current loss: 0.000361887 and accuracy: 1.0\n",
      "Epoch 60, CIFAR-10 Batch 5:  current loss: 0.000465072 and accuracy: 1.0\n",
      "Epoch 61, CIFAR-10 Batch 1:  current loss: 0.00213442 and accuracy: 1.0\n",
      "Epoch 61, CIFAR-10 Batch 2:  current loss: 0.000398181 and accuracy: 1.0\n",
      "Epoch 61, CIFAR-10 Batch 3:  current loss: 0.00222911 and accuracy: 1.0\n",
      "Epoch 61, CIFAR-10 Batch 4:  current loss: 0.00107788 and accuracy: 1.0\n",
      "Epoch 61, CIFAR-10 Batch 5:  current loss: 0.000996484 and accuracy: 1.0\n",
      "Epoch 62, CIFAR-10 Batch 1:  current loss: 0.000733233 and accuracy: 1.0\n",
      "Epoch 62, CIFAR-10 Batch 2:  current loss: 0.0135067 and accuracy: 1.0\n",
      "Epoch 62, CIFAR-10 Batch 3:  current loss: 0.000498189 and accuracy: 1.0\n",
      "Epoch 62, CIFAR-10 Batch 4:  current loss: 0.00336546 and accuracy: 1.0\n",
      "Epoch 62, CIFAR-10 Batch 5:  current loss: 0.00194468 and accuracy: 1.0\n",
      "Epoch 63, CIFAR-10 Batch 1:  current loss: 0.000594726 and accuracy: 1.0\n",
      "Epoch 63, CIFAR-10 Batch 2:  current loss: 0.00134346 and accuracy: 1.0\n",
      "Epoch 63, CIFAR-10 Batch 3:  current loss: 0.00139886 and accuracy: 1.0\n",
      "Epoch 63, CIFAR-10 Batch 4:  current loss: 0.000332645 and accuracy: 1.0\n",
      "Epoch 63, CIFAR-10 Batch 5:  current loss: 0.00354615 and accuracy: 1.0\n",
      "Epoch 64, CIFAR-10 Batch 1:  current loss: 0.000230375 and accuracy: 1.0\n",
      "Epoch 64, CIFAR-10 Batch 2:  current loss: 0.00114879 and accuracy: 1.0\n",
      "Epoch 64, CIFAR-10 Batch 3:  current loss: 0.00265126 and accuracy: 1.0\n",
      "Epoch 64, CIFAR-10 Batch 4:  current loss: 0.0157321 and accuracy: 1.0\n",
      "Epoch 64, CIFAR-10 Batch 5:  current loss: 0.00205703 and accuracy: 1.0\n",
      "Epoch 65, CIFAR-10 Batch 1:  current loss: 0.00011987 and accuracy: 1.0\n",
      "Epoch 65, CIFAR-10 Batch 2:  current loss: 0.00191322 and accuracy: 1.0\n",
      "Epoch 65, CIFAR-10 Batch 3:  current loss: 0.00162808 and accuracy: 1.0\n",
      "Epoch 65, CIFAR-10 Batch 4:  current loss: 0.00100943 and accuracy: 1.0\n",
      "Epoch 65, CIFAR-10 Batch 5:  current loss: 0.00888448 and accuracy: 1.0\n",
      "Epoch 66, CIFAR-10 Batch 1:  current loss: 0.00317147 and accuracy: 1.0\n",
      "Epoch 66, CIFAR-10 Batch 2:  current loss: 0.00488799 and accuracy: 1.0\n",
      "Epoch 66, CIFAR-10 Batch 3:  current loss: 0.00607158 and accuracy: 1.0\n",
      "Epoch 66, CIFAR-10 Batch 4:  current loss: 0.00143687 and accuracy: 1.0\n",
      "Epoch 66, CIFAR-10 Batch 5:  current loss: 0.00154941 and accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.7126791401273885\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsEAAAJ/CAYAAACZcQ5xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIgUjibBIiIiIlI4mgSLiIiISOH8f/67vMfsKBn8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4e4059a58>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 352
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. That's because there are many more techniques that can be applied to your model and we recemmond that once you are done with this project, you explore!\n",
    "\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
